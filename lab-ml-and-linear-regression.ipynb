{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting House Prices in California with `LinearRegression()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab you will start inspect, analyze, visualize house price data from different districts in California, US. After having performed analysis, EDA and some feature engineering, you will build your own `LinearRegression()`  with `SkLearn`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Inspection and Cleaning\n",
    "\n",
    "\n",
    "#### Import and Inspect your data\n",
    "\n",
    "Read the `housing.csv` file and make use of some methods to understand your data better. Below is an explanation of the features you are going to work with:\n",
    "\n",
    "1. **longitude:**  geographical coordinate, east to west position of district\n",
    "2. **latitude:**  geographical coordinate, north to south position of district\n",
    "3. **housing_median_age:** the median age of houses in district\n",
    "4. **total_rooms** Sum of all rooms in district\n",
    "5. **total_bedrooms** Sum of all bedrooms in district\n",
    "6. **population:** total population in district\n",
    "7. **households:** total households in district\n",
    "8. **median_income:** median household income in district \n",
    "9. **median_house_value:** median house value in district\n",
    "10. **ocean_proximity:** District´s proximity to the ocean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset using the recommended method\n",
    "file_path = '../data/housing.csv'  # Replace with the correct path if different\n",
    "housing_data = pd.read_csv(file_path, sep='\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the correct column names for the dataset\n",
    "column_names = [\n",
    "    \"longitude\", \"latitude\", \"housing_median_age\", \"total_rooms\",\n",
    "    \"total_bedrooms\", \"population\", \"households\", \"median_income\",\n",
    "    \"median_house_value\", \"ocean_proximity\"\n",
    "]\n",
    "\n",
    "# Fill missing values with column means\n",
    "X = X.fillna(X.mean())\n",
    "# Check if the number of columns matches the expected number\n",
    "if housing_data.shape[1] < len(column_names):\n",
    "    raise ValueError(f\"Dataset has fewer columns ({housing_data.shape[1]}) than expected ({len(column_names)}).\")\n",
    "elif housing_data.shape[1] > len(column_names):\n",
    "    print(f\"Dataset has more columns ({housing_data.shape[1]}) than expected ({len(column_names)}). Extra columns will be ignored.\")\n",
    "\n",
    "# Create a new dataset with the correct column names\n",
    "housing_data_cleaned = housing_data.iloc[:, :len(column_names)]  # Select only the relevant columns\n",
    "housing_data_cleaned.columns = column_names\n",
    "\n",
    "# Display information about the cleaned dataset\n",
    "print(\"\\nFirst Few Rows of the Dataset:\")\n",
    "print(housing_data_cleaned.head())  # Show first 5 rows\n",
    "\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(housing_data_cleaned.describe())  # Summary statistics for numeric columns\n",
    "\n",
    "# Check the distribution of unique values in each column (for categorical or numeric variables)\n",
    "print(\"\\nUnique Values per Column:\")\n",
    "for col in housing_data_cleaned.columns:\n",
    "    print(f\"{col}: {housing_data_cleaned[col].nunique()} unique values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "Make histograms of all your numeric columns in order to get a good understanding of the distribution of your data points. What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for all numeric columns in the cleaned dataset\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select numeric columns\n",
    "numeric_columns = housing_data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Plot histograms for numeric features\n",
    "housing_data_cleaned[numeric_columns].hist(bins=30, figsize=(14, 10), edgecolor='black')\n",
    "plt.suptitle(\"Histograms of Numeric Features\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations:\n",
    "- __Longitude and Latitude__: These geographical features may show a uniform distribution or clusters based on the locations in California.\n",
    "- __Housing Median Age__: Typically, this will be slightly right-skewed, with most houses being relatively new.\n",
    "- __Total Rooms, Total Bedrooms, Population, Households__: These features are often heavily right-skewed, as most districts have smaller counts with a few outliers having very high values.\n",
    "- __Median Income__: This may show a normal-like distribution but skewed towards higher or lower income levels depending on the area's economic conditions.\n",
    "- __Median House Value__: Likely right-skewed, as most house prices cluster towards lower values with a few outliers in high-value districts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's create some features a tidy up our data\n",
    "\n",
    "1. Locate your NaN values and make a decision on how to handle them. Drop, fill with mean, or something else, it is entirely up to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMissing Values:\")\n",
    "print(housing_data_cleaned.info())  # Shows data types, non-null counts, and memory usage\n",
    "\n",
    "# Check for missing values in the dataset\n",
    "nan_summary = housing_data_cleaned.isnull().sum()\n",
    "\n",
    "# Handle missing values: Numeric columns\n",
    "numeric_columns = housing_data_cleaned.select_dtypes(include=['float64', 'int64']).columns\n",
    "housing_data_cleaned[numeric_columns] = housing_data_cleaned[numeric_columns].fillna(\n",
    "    housing_data_cleaned[numeric_columns].mean()\n",
    ")\n",
    "\n",
    "# Handle missing values: Categorical columns\n",
    "categorical_columns = housing_data_cleaned.select_dtypes(include=['object']).columns\n",
    "if len(categorical_columns) > 0:\n",
    "    housing_data_cleaned[categorical_columns] = housing_data_cleaned[categorical_columns].fillna(\n",
    "        housing_data_cleaned[categorical_columns].mode().iloc[0]\n",
    "    )\n",
    "\n",
    "# Recheck for missing values to confirm handling\n",
    "nan_summary_after = housing_data_cleaned.isnull().sum()\n",
    "\n",
    "print(\"\\nMissing Values After Handling:\")\n",
    "print(nan_summary_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Create three new columns by using simple arithmetic operations. Create one column with \"rooms per household\", one with \"population per household\",  and one with \"bedrooms per room\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create new columns based on arithmetic operations\n",
    "housing_data_cleaned[\"rooms_per_household\"] = housing_data_cleaned[\"total_rooms\"] / housing_data_cleaned[\"households\"]\n",
    "housing_data_cleaned[\"population_per_household\"] = housing_data_cleaned[\"population\"] / housing_data_cleaned[\"households\"]\n",
    "\n",
    "housing_data_cleaned[\"bedrooms_per_room\"] = np.where(\n",
    "    housing_data_cleaned[\"total_rooms\"] == 0,\n",
    "    0,  # Asignar NaN donde total_rooms es 0\n",
    "    housing_data_cleaned[\"total_bedrooms\"] / housing_data_cleaned[\"total_rooms\"]\n",
    ")\n",
    "\n",
    "# Display the first few rows to verify the new columns\n",
    "print(\"Dataset with New Columns:\")\n",
    "print(housing_data_cleaned[[\"rooms_per_household\", \"population_per_household\", \"bedrooms_per_room\"]].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you check the largest and smallest values of your \"rooms per houshold column\" you will see two outliers and two values that are just wrong. Drop the four values by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recreate the \"rooms_per_household\" column if it does not exist\n",
    "if \"rooms_per_household\" not in housing_data_cleaned.columns:\n",
    "    housing_data_cleaned[\"rooms_per_household\"] = housing_data_cleaned[\"total_rooms\"] / housing_data_cleaned[\"households\"]\n",
    "\n",
    "# Check the largest and smallest values in the \"rooms_per_household\" column\n",
    "largest_values = housing_data_cleaned[\"rooms_per_household\"].nlargest(2)\n",
    "smallest_values = housing_data_cleaned[\"rooms_per_household\"].nsmallest(2)\n",
    "\n",
    "print(\"Largest Values (Outliers):\")\n",
    "print(largest_values)\n",
    "\n",
    "print(\"\\nSmallest Values (Outliers):\")\n",
    "print(smallest_values)\n",
    "\n",
    "# Drop the rows with the identified outliers by index\n",
    "outlier_indices = largest_values.index.tolist() + smallest_values.index.tolist()\n",
    "housing_data_cleaned = housing_data_cleaned.drop(index=outlier_indices)\n",
    "\n",
    "# Verify the updated dataset\n",
    "print(\"\\nUpdated Dataset after Removing Outliers:\")\n",
    "print(housing_data_cleaned[\"rooms_per_household\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps Taken:\n",
    "- Check for Missing Column: If \"rooms_per_household\" is missing, it will be recreated before proceeding.\n",
    "- Identify Outliers: Used nlargest(2) and nsmallest(2) to identify extreme values.\n",
    "- Drop Outliers: Combined the indices of the largest and smallest values and removed them from the dataset.\n",
    "\n",
    "This ensures the column is always available for processing. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Exploratory Data Analysis\n",
    "\n",
    "\n",
    "\n",
    "#### Let's find out what factors have an influence on our predicting variable\n",
    "\n",
    "1. Let's check out the distribution of our \"median house value\". Visualize your results with 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of \"median_house_value\" with 100 bins\n",
    "housing_data_cleaned[\"median_house_value\"].hist(bins=100, figsize=(10, 6), edgecolor=\"black\")\n",
    "plt.title(\"Distribution of Median House Value\", fontsize=16)\n",
    "plt.xlabel(\"Median House Value\", fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histogram shows the distribution of the median_house_value variable with 100 bins.\n",
    "\n",
    "### Observations:\n",
    "- Right Skewed: The distribution appears slightly right-skewed, with most house values clustered on the lower end of the spectrum.\n",
    "- Capping: There might be a cap in the data at certain points (e.g., at the higher end), suggesting that some values could be artificially limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check out what variables correlates the most with \"median house value\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Seleccionar solo las columnas numéricas del DataFrame\n",
    "numeric_data = housing_data_cleaned.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "\n",
    "# Calcular la matriz de correlación\n",
    "correlation_matrix = numeric_data.corr()\n",
    "\n",
    "# Crear un heatmap para visualizar las correlaciones\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar=True)\n",
    "plt.title(\"Correlation Heatmap\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the top correlations with \"median_house_value\"\n",
    "top_correlations = correlation_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "print(\"Top Correlations with Median House Value:\")\n",
    "print(top_correlations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Correlation Heatmap:\n",
    "    - Visualizes how strongly features are correlated with one another and with the target variable (median_house_value).\n",
    "    - Positive correlations (close to +1) indicate a direct relationship, while negative correlations (close to -1) show an inverse relationship.\n",
    "- Top Correlations:\n",
    "    - ocean_proximity, longitude, and total_bedrooms are the most positively correlated features with median_house_value.\n",
    "    - Features like latitude and population show a weaker, negative correlation with house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's check out the distribution of the column that has the highest correlation to \"median house value\". Visualize your results with 100 bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculate correlations to identify the column with the highest correlation to \"median_house_value\"\n",
    "correlation_matrix = housing_data_cleaned.corr()\n",
    "top_correlations = correlation_matrix[\"median_house_value\"].sort_values(ascending=False)\n",
    "\n",
    "# Identify the column with the highest correlation to \"median_house_value\" (excluding itself)\n",
    "highest_corr_column = top_correlations.index[1]  # Skip the target itself\n",
    "\n",
    "# Plot the distribution of the column with the highest correlation\n",
    "housing_data_cleaned[highest_corr_column].hist(bins=100, figsize=(10, 6), edgecolor=\"black\")\n",
    "plt.title(f\"Distribution of {highest_corr_column}\", fontsize=16)\n",
    "plt.xlabel(highest_corr_column, fontsize=14)\n",
    "plt.ylabel(\"Frequency\", fontsize=14)\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Skewness or Clustering:\n",
    "    - If the variable is categorical (e.g., ocean_proximity), the histogram will show distinct groups based on the categories.\n",
    "    - If it’s numeric, you can observe whether the data is skewed, normally distributed, or contains outliers.\n",
    "- Data Patterns:\n",
    "    - This visualization helps in understanding how the most important feature behaves and whether transformations or further cleaning are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Visualize the \"median house value\" and \"median income\" in a jointplot (kind=\"reg\"). What do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Seaborn for visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a jointplot for \"median_house_value\" and \"median_income\" with a regression line\n",
    "sns.jointplot(\n",
    "    data=housing_data_cleaned,\n",
    "    x=\"median_income\",\n",
    "    y=\"median_house_value\",\n",
    "    kind=\"reg\",\n",
    "    height=8\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.suptitle(\"Jointplot of Median House Value vs Median Income\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Positive Correlation:\n",
    "    - The scatterplot shows a clear positive correlation between median_income and median_house_value.\n",
    "    - As income increases, house value tends to increase.\n",
    "- Capping:\n",
    "    - There appears to be a cap on median_house_value, likely at the higher end (e.g., 500,000). This suggests the data may have a threshold or ceiling.\n",
    "- Regression Line:\n",
    "    - The regression line indicates a strong linear trend between the two variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Make the same visualization as in the above, but, cahnge the kind parameter to \"kde\". What extra information does this type of visualization convey, that the one in the above does not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a jointplot for \"median_house_value\" and \"median_income\" with KDE visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sns.jointplot(\n",
    "    data=housing_data_cleaned,\n",
    "    x=\"median_income\",\n",
    "    y=\"median_house_value\",\n",
    "    kind=\"kde\",\n",
    "    height=8\n",
    ")\n",
    "\n",
    "# Display the plot\n",
    "plt.suptitle(\"Jointplot of Median House Value vs Median Income (KDE)\", y=1.02, fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Information from KDE Visualization:\n",
    "- Density Patterns:\n",
    "    - The KDE plot shows areas of high density where the data points are concentrated, which may not be as obvious in a scatterplot.\n",
    "    - The darker regions indicate where most data points are clustered.\n",
    "- Outlier Insights:\n",
    "    - Unlike the scatterplot, KDE smooths the data, making it easier to spot clusters and less impacted by individual outliers.\n",
    "- Relationship Overview:\n",
    "    - The KDE gives a broader view of the relationship, highlighting trends and distributions, rather than focusing on individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's get schwifty with some EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a new categorical column from the \"median income\" with the following quartiles `[0, 0.25, 0.5, 0.75, 0.95, 1]` and label them like this `[\"Low\", \"Below_Average\", \"Above_Average\", \"High\", \"Very High\"]` and name the column \"income_cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new categorical column \"income_cat\" based on the quartiles of \"median_income\"\n",
    "housing_data_cleaned[\"income_cat\"] = pd.qcut(\n",
    "    housing_data_cleaned[\"median_income\"],\n",
    "    q=[0, 0.25, 0.5, 0.75, 0.95, 1],  # Define the quantile ranges\n",
    "    labels=[\"Low\", \"Below_Average\", \"Above_Average\", \"High\", \"Very High\"]  # Labels for the categories\n",
    ")\n",
    "\n",
    "# Display the first few rows of the new column to verify\n",
    "print(\"New Column 'income_cat':\")\n",
    "print(housing_data_cleaned[[\"median_income\", \"income_cat\"]].head())\n",
    "\n",
    "# Check the distribution of the new \"income_cat\" column\n",
    "print(\"\\nDistribution of Income Categories:\")\n",
    "print(housing_data_cleaned[\"income_cat\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Income Categories:\n",
    "    - The dataset now includes an income_cat column that categorizes median_income into:\n",
    "        - Low: Bottom 25%.\n",
    "        - Below_Average: 25th–50th percentile.\n",
    "        - Above_Average: 50th–75th percentile.\n",
    "        - High: 75th–95th percentile.\n",
    "        - Very High: Top 5%.\n",
    "- Distribution:\n",
    "    - The value counts show how many rows fall into each category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Using the Seaborn library, plot the count of your new column and set the `hue` to \"ocean_proximity\". What interesting things can you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the count of the new column \"income_cat\" with hue set to \"ocean_proximity\"\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.countplot(\n",
    "    data=housing_data_cleaned,\n",
    "    x=\"income_cat\",\n",
    "    hue=\"ocean_proximity\",\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Income Categories Count by Ocean Proximity\", fontsize=16)\n",
    "plt.xlabel(\"Income Category\", fontsize=14)\n",
    "plt.ylabel(\"Count\", fontsize=14)\n",
    "plt.legend(title=\"Ocean Proximity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Distribution by Income Categories:\n",
    "    - The counts vary significantly across income categories, with more lower-income categories being present.\n",
    "    - Certain income categories (e.g., \"Very High\") have fewer districts.\n",
    "- Effect of Ocean Proximity:\n",
    "    - Near Ocean and Near Bay regions appear to have higher representation in the higher income categories.\n",
    "    - Inland regions dominate the lower income categories, indicating that proximity to the ocean correlates with higher income."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Create two barplots where you set \"y=\"median_house_value\" on both, and the x is first \"income cat\" and then \"ocean_proximity\". How does these two graphs complement what you saw in the graph in your previous question?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Barplot 1: Median house value by income category\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(\n",
    "    data=housing_data_cleaned,\n",
    "    x=\"income_cat\",\n",
    "    y=\"median_house_value\",\n",
    "    hue=\"income_cat\",  # Set hue to the same as x for consistent coloring\n",
    "    dodge=False,  # Disable dodge to align bars\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Median House Value by Income Category\", fontsize=16)\n",
    "plt.xlabel(\"Income Category\", fontsize=14)\n",
    "plt.ylabel(\"Median House Value\", fontsize=14)\n",
    "plt.legend([], [], frameon=False)  # Disable legend\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print()\n",
    "\n",
    "# Barplot 2: Median house value by ocean proximity\n",
    "plt.figure(figsize=(15, 6))\n",
    "sns.barplot(\n",
    "    data=housing_data_cleaned,\n",
    "    x=\"ocean_proximity\",\n",
    "    y=\"median_house_value\",\n",
    "    hue=\"ocean_proximity\",  # Set hue to x for consistent coloring\n",
    "    dodge=False,\n",
    "    palette=\"viridis\"\n",
    ")\n",
    "plt.title(\"Median House Value by Ocean Proximity\", fontsize=16)\n",
    "plt.xlabel(\"Ocean Proximity\", fontsize=14)\n",
    "plt.ylabel(\"Median House Value\", fontsize=14)\n",
    "plt.legend([], [], frameon=False)  # Disable legend\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Barplot 1: Median House Value by Income Category:\n",
    "    - The median_house_value increases significantly as income categories rise.\n",
    "    - The \"Very High\" income category shows the highest house prices, confirming the strong correlation between income and house prices.\n",
    "- Barplot 2: Median House Value by Ocean Proximity:\n",
    "    - House values are highest in areas with Ocean Proximity.\n",
    "    - Inland areas show the lowest median house values, reinforcing the impact of proximity to the ocean on house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Create a pivoted dataframe where you have the values of the \"income cat\" column as indices and the values of the \"ocean_proximity\" column as columns. Also drop the \"ISLAND\" column that you'll get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pivot table using \"income_cat\" as index and \"ocean_proximity\" as columns\n",
    "pivot_table = housing_data_cleaned.pivot_table(\n",
    "    index=\"income_cat\",\n",
    "    columns=\"ocean_proximity\",\n",
    "    values=\"median_house_value\",\n",
    "    aggfunc=\"mean\",\n",
    "    observed=False  # Explicitly specify observed=False to match current behavior\n",
    ")\n",
    "\n",
    "# Drop the \"ISLAND\" column if it exists\n",
    "if \"ISLAND\" in pivot_table.columns:\n",
    "    pivot_table = pivot_table.drop(columns=[\"ISLAND\"])\n",
    "\n",
    "# Display the pivoted dataframe\n",
    "print(\"Pivoted DataFrame (Mean Median House Value):\")\n",
    "print(pivot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Income Categories: Rows represent the different income_cat categories (Low, Below_Average, etc.).\n",
    "- Ocean Proximity: Columns represent proximity to the ocean (e.g., Near Ocean, Inland).\n",
    "- Values: The table shows the average median_house_value for each income category and ocean proximity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Turn your pivoted dataframe into a heatmap. The heatmap should have annotations in integer format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Seaborn for heatmap visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a heatmap from the pivot table\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(\n",
    "    pivot_table,\n",
    "    annot=True,  # Add annotations\n",
    "    fmt=\".0f\",  # Format annotations as integers\n",
    "    cmap=\"coolwarm\",  # Use a diverging color palette\n",
    "    cbar=True  # Show color bar\n",
    ")\n",
    "\n",
    "# Add title and axis labels\n",
    "plt.title(\"Heatmap of Median House Value by Income Category and Ocean Proximity\", fontsize=16)\n",
    "plt.xlabel(\"Ocean Proximity\", fontsize=14)\n",
    "plt.ylabel(\"Income Category\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Color Gradient:\n",
    "    - The heatmap uses the coolwarm palette to show how the median_house_value changes across income_cat and ocean_proximity.\n",
    "    - Higher values are represented by warmer colors (reds), and lower values by cooler colors (blues).\n",
    "- Annotations:\n",
    "    - The exact average house value for each combination is displayed, rounded to the nearest integer.\n",
    "- Insights:\n",
    "    - House prices increase significantly with higher income categories.\n",
    "    - Regions closer to the ocean consistently show higher house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Preparing your Data\n",
    "\n",
    "\n",
    "\n",
    "#### Splitting, Preparing and Engineering some Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Let's drop the \"income_cat\" column as it has served its purpose already. We don't need for our model as we already have \"median income\".\n",
    "Not dropping \"incom cat\" will lead to multicolinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import Index\n",
    "\n",
    "# Crear un objeto Index manualmente\n",
    "columns_index = Index(['longitude', 'latitude', 'housing_median_age', 'total_rooms',\n",
    "                       'total_bedrooms', 'population', 'households', 'median_income',\n",
    "                       'median_house_value', 'ocean_proximity'], dtype='object')\n",
    "\n",
    "print(columns_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Select your floating point columns and standardize your data by calculating the Z-score. You can apply the `stats.zscore()` method in a lambda function. Save your results to a variable called `z_scored`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.stats import zscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select floating-point columns\n",
    "float_columns = housing_data_cleaned.select_dtypes(include=['float64']).columns\n",
    "\n",
    "# Standardize the data by calculating the Z-score for each numeric column\n",
    "z_scored = housing_data_cleaned[float_columns].apply(zscore, nan_policy='omit')\n",
    "\n",
    "# Display the first few rows of the standardized data\n",
    "print(\"Z-Scored Data:\")\n",
    "print(z_scored.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- The floating-point columns have been standardized using Z-scores.\n",
    "- Z-scores represent the number of standard deviations a data point is from the mean.\n",
    "- This process ensures that all floating-point features are on the same scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Turn the only categorical columns into dummies. Be vary of the dummy trap, to avoid multicolinearity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the categorical column \"ocean_proximity\" into dummy variables while avoiding the dummy trap\n",
    "housing_data_encoded = pd.get_dummies(\n",
    "    housing_data_cleaned,\n",
    "    columns=[\"ocean_proximity\"],  # Specify the categorical column to encode\n",
    "    drop_first=True  # Avoid the dummy trap by dropping the first category\n",
    ")\n",
    "\n",
    "# Display the first few rows of the encoded dataset\n",
    "print(\"Dataset with Dummy Variables:\")\n",
    "print(housing_data_encoded.head())\n",
    "\n",
    "# Verify the new columns\n",
    "print(\"\\nColumns after encoding:\")\n",
    "print(housing_data_encoded.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Save our predicting variable to `y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the predicting variable (target) to 'y'\n",
    "y = housing_data_encoded[\"median_house_value\"]\n",
    "\n",
    "# Display the first few rows of 'y' to verify\n",
    "print(\"Predicting Variable (y):\")\n",
    "print(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Concatenate `z_scored` and `dummies` and drop the predicting variable. Save to the varible `X`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asegúrate de que 'z_scored' sea un DataFrame antes de concatenar\n",
    "if not isinstance(z_scored, pd.DataFrame):\n",
    "    z_scored = pd.DataFrame(z_scored, columns=[\"z_scored\"], index=housing_data_encoded.index)\n",
    "\n",
    "# Concatenar z_scored con los datos codificados (excluyendo 'median_house_value')\n",
    "X = pd.concat([z_scored, housing_data_encoded.drop(columns=[\"median_house_value\"])], axis=1)\n",
    "\n",
    "# Mostrar las primeras filas de la matriz de características (X) para verificar\n",
    "print(\"Feature Matrix (X):\")\n",
    "print(X.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 - Machine Learning \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Train, Test, Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Import `train_test_split` and split your data accordingly. Choose an appropriate test size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train_test_split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# Set test_size to 20% of the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"Shapes of Training and Testing Sets:\")\n",
    "print(f\"X_train: {X_train.shape}, X_test: {X_test.shape}\")\n",
    "print(f\"y_train: {y_train.shape}, y_test: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building and Training our Model\n",
    "\n",
    "2. Build, fit and train a `LinearRegression` model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns in X are numeric\n",
    "X = pd.concat([z_scored, housing_data_encoded.drop(columns=[\"median_house_value\"])], axis=1)\n",
    "\n",
    "# Check for non-numeric columns\n",
    "non_numeric_columns = X.select_dtypes(exclude=[\"float64\", \"int64\"]).columns\n",
    "print(\"Non-numeric columns:\", non_numeric_columns)\n",
    "\n",
    "# Drop or encode non-numeric columns if any exist\n",
    "if len(non_numeric_columns) > 0:\n",
    "    X = X.drop(columns=non_numeric_columns)\n",
    "\n",
    "# Split the cleaned data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the Linear Regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Display model coefficients and intercept\n",
    "print(\"Model Coefficients:\")\n",
    "print(model.coef_)\n",
    "print(\"\\nModel Intercept:\")\n",
    "print(model.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. In a scatterplot, visualize the y_train on your x-axis and your predictions on the y-axis. How does your training predictions look? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions for the training set\n",
    "y_train_predictions = model.predict(X_train)\n",
    "\n",
    "# Create a scatterplot to visualize y_train vs. predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_train, y_train_predictions, alpha=0.5)\n",
    "plt.title(\"Scatterplot of Actual vs. Predicted (Training Set)\", fontsize=16)\n",
    "plt.xlabel(\"Actual Values (y_train)\", fontsize=14)\n",
    "plt.ylabel(\"Predicted Values\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. From the sklearn metrics module, print the mean_squared_error and R^2-score. What does the metrics tell us?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is split and prepared\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Re-split the dataset if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Re-train the model if needed\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the training set\n",
    "y_train_predictions = model.predict(X_train)\n",
    "\n",
    "# Calculate metrics for the training set\n",
    "mse = mean_squared_error(y_train, y_train_predictions)\n",
    "r2 = r2_score(y_train, y_train_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n",
    "print(f\"R^2 Score: {r2:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics Interpretation:\n",
    "- MSE (Mean Squared Error):\n",
    "    - Indicates the average squared difference between the predicted and actual values.\n",
    "    - Lower values indicate better fit.\n",
    "- R2 Score:\n",
    "    - Explains the proportion of variance in the dependent variable that is predictable from the independent variables.\n",
    "    - Values close to 1 indicate a good fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Predictions\n",
    "\n",
    "1. Now you are ready to make prediction on the test data. Do that and visualize your results in a new scatterplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure data is split and prepared\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the dataset again if needed\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_test_predictions = model.predict(X_test)\n",
    "\n",
    "# Create a scatterplot to visualize y_test vs. predictions\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, y_test_predictions, alpha=0.5)\n",
    "plt.title(\"Scatterplot of Actual vs. Predicted (Test Set)\", fontsize=16)\n",
    "plt.xlabel(\"Actual Values (y_test)\", fontsize=14)\n",
    "plt.ylabel(\"Predicted Values\", fontsize=14)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Print the mean_squared_error and R^2-score again. What has happened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split the data again\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the Linear Regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_test_predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate metrics for the test set\n",
    "mse_test = mean_squared_error(y_test, y_test_predictions)\n",
    "r2_test = r2_score(y_test, y_test_predictions)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Mean Squared Error (MSE) on Test Set: {mse_test:.2f}\")\n",
    "print(f\"R^2 Score on Test Set: {r2_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- Dataset Split: Ensures X_train, X_test, y_train, and y_test are properly defined.\n",
    "- Model Training: Fits the Linear Regression model on the training set.\n",
    "- Metrics Calculation: Computes the MSE and R² score on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. There is another metric called Root mean squared error, Which is the square root of the MSE. Calculate the RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated method for RMSE calculation to avoid deprecation warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE directly using numpy's sqrt on MSE\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predictions))\n",
    "\n",
    "# Print RMSE\n",
    "print(f\"Root Mean Squared Error (RMSE) on Test Set: {rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Questions 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Create a dataframe with two columns, one consisting of the y_test and one of your model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the updated method for RMSE calculation to avoid deprecation warnings\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calculate RMSE directly using numpy's sqrt on MSE\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predictions))\n",
    "\n",
    "# Print RMSE\n",
    "print(f\"Root Mean Squared Error (RMSE) on Test Set: {rmse_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make a series of of your new dataframe, by calculating the predicted error in absolut numbers. Save this series to variable name `absolute_errors`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure necessary imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Ensure X and y are defined\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the test set\n",
    "y_test_predictions = model.predict(X_test)\n",
    "\n",
    "# Create a DataFrame with actual and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    \"Actual Values\": y_test,\n",
    "    \"Predicted Values\": y_test_predictions\n",
    "})\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "print(\"Comparison of Actual vs. Predicted Values:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. If you take the mean of your series, you will get the mean absolute errors, which is another metric for Linear Regressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus Question 2 - Build a Random Forest Regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Build, fit and train a `RandomForestRegressor` model. Do this by following the same staps that you followed when building your `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'path_to_your_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Step 2: Load and Prepare Data\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Replace 'path_to_your_data.csv' with your actual data file\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_your_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Assuming 'target' is the column you want to predict\u001b[39;00m\n\u001b[1;32m     13\u001b[0m X \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Features\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'path_to_your_data.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Step 2: Load and Prepare Data\n",
    "# Replace 'path_to_your_data.csv' with your actual data file\n",
    "data = pd.read_csv('path_to_your_data.csv')\n",
    "\n",
    "# Assuming 'target' is the column you want to predict\n",
    "X = data.drop('target', axis=1)  # Features\n",
    "y = data['target']  # Target variable\n",
    "\n",
    "# Step 3: Split Data into Training and Test Sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Build and Fit the RandomForestRegressor Model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: Make Predictions on the Test Data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Step 6: Evaluate the Model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R² Score: {r2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Make prediction on the test data and evaluate you results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
